---
title: "AM10 Group Project 1"
author: "Group 1: Alex Scheuer, Dhruvi Mundra, Heng Jian Shun, Marta Wnek, Sharon Wenyu Xu, Xueying Liu"
date: "`r Sys.Date()`"
output: html_document
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r, load_libraries, include = FALSE}
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate) # to handle dates
library(GGally) # for correlation-scatter plot matrix
library(ggfortify) # to produce residual diagnostic plots
library(janitor) # clean_names()
library(broom) # use broom:augment() to get tidy table with 
library(extrafont)
library(vroom)
library(ggtext)
library(gapminder)
library(ggrepel)
library(patchwork)
library(gghighlight)
library(skimr)

library(rsample) # to split dataframe in training- & testing sets
library(huxtable) # to get summary table of all models produced
library(caret) # to train more advanced models (k-fold cross-validation, stepwise regression, LASSO)
library(nnet) # to calculate the maximum value of a vector
library(pROC) # to plot ROC curves
library(MLmetrics) #for caret LASSO logistic regression
library(DescTools)
library(sampling)
library(gbm)
library(factoextra)


library(hrbrthemes)
library(viridis)
library(likert) #to have double sided bar charts
```

# Introduction

## Load the data

Dataset source: <https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020>

```{r, load_data, warning=FALSE, message=FALSE}
animeinfo_raw <- read_csv(here::here("anime.csv")) %>%
  clean_names() # use janitor::clean_names()

# animerating_raw <- read_csv(here::here("animelist.csv")) %>%
#   clean_names() # use janitor::clean_names()
```

# ICE the data: Inspect, Clean, Explore

Any data science engagement starts with ICE. Inspecting, Clean and Explore the data.

## Inspect the data

Inspect the data to understand what different variables mean. Variable definitions can be found in the excel version of the data.

### Anime List data

```{r, Inspect anime list data}
glimpse(animeinfo_raw)
skimr::skim(animeinfo_raw)

# Check through each factor columns and inspect number of unique factors.
unique(animelist$type)
unique(animelist$source)
unique(animelist$rating)
# These columns are fine - There are distinct categories for each

```

### Anime Ratings data

```{r, Inspect anime ratings data}
glimpse(animerating_raw)
skimr::skim(animerating_raw)

```

## Clean the data

-   Clean names

-   Remove redundant columns

-   Remove rows that contain cells with missing values for score since that is the most important one

-   Change variable types where needed

-   Isolate date in which anime started airing

-   Create indicator variables for year, month, week, day for "aired from"

-   Re-calculate duration of anime as minutes

-   Extract date premiered in terms of year and season

-   Extract multi-value columns (genres, licensors, producers, studios) and create binary columns for each

### Clean Anime List data

```{r, clean anime list data, warning=FALSE}
animelist_intermediate <- animeinfo_raw %>%
  select(-c(english_name, japanese_name, ranked)) %>% #drop redundant columns and perfectly multicollinear column ranked %>% 
  select(-c(score_10 : score_1)) %>% #drop redundant columns
  dplyr::mutate(across(c(score, episodes), as.numeric)) %>% #convert to numeric
  drop_na(score) %>% #drop rows with "Unknown" scores
  dplyr::mutate(aired_from = str_split(aired, "to", simplify = T)[, 1]) %>% 
  mutate(
    aired_from = mdy(aired_from),
    aired_from_year = year(aired_from)
    ) %>% #cleaned aired_from and retain aired from year only
  select(-c(aired, aired_from)) %>% #drop aired
  mutate(droprate = 100*dropped/(dropped+watching+completed+on_hold)) %>% 
  mutate(
    duration_hour = str_extract(duration, "(\\d)+(?= hr)"),
    duration_hour = ifelse(is.na(duration_hour), 0, as.numeric(duration_hour)),
    duration_min = as.numeric(str_extract(duration, "(\\d)+(?= min)")),
    duration = (duration_hour * 60) + duration_min #cleaned duration - number of minutes
    ) %>%
  select(-c(duration_hour, duration_min)) %>% #drop duration_hour and duration_min
  select(-c(premiered)) #drop premiered
  
glimpse(animelist_intermediate)
  

########################
# Replace (creating binary columns) and remove multi-valued columns (genres, licensors, producers, studios)
animelist_clean <- animelist_intermediate %>% 
  mutate(genre = strsplit(genres, ", ")) %>%
  unnest(genre) %>% 
  arrange(genre) %>%  
  pivot_wider(names_from = genre,
              names_prefix = "genre_", names_repair = "universal",
              values_from = genre, values_fill = 0, values_fn = length) %>% #binary genres
  
  mutate(licensor = strsplit(licensors, ", ")) %>%
  unnest(licensor) %>% 
  arrange(licensor) %>%  
  pivot_wider(names_from = licensor,
              names_prefix = "licensor_", names_repair = "universal",
              values_from = licensor, values_fill = 0, values_fn = length) %>% #binary licensors
  
  mutate(producer = strsplit(producers, ", ")) %>%
  unnest(producer) %>% 
  arrange(producer) %>%  
  pivot_wider(names_from = producer,
              names_prefix = "producer_", names_repair = "universal",
              values_from = producer, values_fill = 0, values_fn = length) %>% #binary producers
  
  mutate(studio = strsplit(studios, ", ")) %>%
  unnest(studio) %>% 
  arrange(studio) %>%  
  pivot_wider(names_from = studio,
              names_prefix = "studio_", names_repair = "universal",
              values_from = studio, values_fill = 0, values_fn = length) %>% #binary studios
  clean_names() %>% 
  select(-c(genres, licensors, producers, studios)) %>%  #drop multivalue columns
  arrange(mal_id) #arrange by anime id for ease
  

rm(animelist_intermediate)
```

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAaCAYAAADFTB7LAAAAa0lEQVR42u3OywnAIBBAwcXSUoCW5D11xDoNCBGNv0MOecJOBSOi1OZMsJ4dvFxEJ1OQnMxBarIKEpNNkJbsBknJYZCSnAYJyVVQziNig7/nZkFEbhTE5HpBVO4dxOXKIDL3BLG5BJ1T6rsbMfep2CaMN00AAAAASUVORK5CYII= "Run Current Chunk")

### Clean Anime Ratings data

```{r, clean anime ratings data, warning=FALSE}
# Store all unique anime IDs as list
uniqueid <- unique(animelist_clean$mal_id)

# rating_clean <- animerating_raw %>% 
#   filter(anime_id %in% uniqueid)

```

## Explore

```{r watchtime  by genre}

# options(scipen=999)
# 
# aaa <- animelist_clean_long %>%
#   drop_na(score, duration, episodes, completed) %>% 
#   group_by(genre, type) %>% 
#   mutate(watch_time = mean(duration * episodes * completed, na.rm = TRUE)) %>%
#   mutate(avg_score = mean(score, na.rm = TRUE)) %>% 
#   ungroup() %>%
#   select(avg_score, type, genre, watch_time, duration, episodes, completed) %>% 
#   distinct(genre, type, .keep_all = TRUE)
# 
# ggplot(aaa) +
#   geom_point(aes(x = watch_time, y = avg_score, colour = type)) +
#   facet_wrap(~type, scale = "free")

# 
# watchtime_by_genre <- animelist_clean_long %>%
#   group_by(genre) %>% 
#   mutate(watch_time = mean(duration * episodes * completed, na.rm = TRUE)) %>%
#   ungroup() %>%
#   select(score, type, watch_time, duration, episodes, completed) %>% 
#   distinct(genre, .keep_all = TRUE) %>% 
#   ungroup() %>% 
#   slice_max(watch_time, n = 20) %>% 
#   group_by_gen
#   
#   
#   
#   group_by(genre, type) %>% 
#   summarise(watch_time = mean(duration*episodes*completed)) %>% 
#   # group_by(genre) %>% 
#   # mutate(genre = fct_reorder(genre, watch_time)) %>% 
#   # ungroup() %>% 
#   # group_by_(genre, type) %>% 
#   ungroup() %>% 
#   slice_max(watch_time, n = 20) %>%
#   mutate(genre = str_replace(genre, "_", " ")) %>%
#   mutate(genre = str_to_title(genre))
# plot3 <- ggplot(data = watchtime_by_genre, aes(x = watch_time, y = fct_reorder(genre, watch_time), fill = type)) +
#   geom_col() +
#   labs(
#     title = "20 most watched Anime genres by type",
#     x = "Watchtime of animes") +
#   theme_classic(base_family = "Times New Roman") +
#   theme(plot.title.position = "plot",
#         plot.title = element_textbox_simple(size = 14),
#         axis.title.y = element_blank(),
#         ) +
#   scale_fill_manual(values = c('#0056a7', '#126cb6', '#2681c5', '#3a97d3', '#52ace0', '#6dc2ec', '#8bd7f5', '#aeecfc', '#d6ffff'))
# plot(plot3)


```

# Create long data on genre

```{r create long animelist}
animelist_clean_long <- animelist_clean %>%
  pivot_longer(cols = c(genre_action : genre_yuri), names_to = "genre", values_to = "binary") %>%
  relocate(genre, .before = score) %>% 
  filter(binary == 1) %>% 
  select(c(mal_id : droprate), binary) %>% 
  mutate(id = row_number()) %>% 
  relocate(id, .before = mal_id) %>%
  mutate(genre = str_extract(genre, '(?<=_).+')) %>% 
  filter(genre != "unknown") #Remove unknown genres
```

## Create correlation matrix

```{r ggcorr plots}
#add a ggcor graph to test for correlation issues
#include only the relevant variables that we intend to build a model on
ggcorr_anime_data <- animelist_clean_long %>% 
  dplyr::select(c(genre:droprate))


#plot the ggcor graphic
ggcor_anime <- ggcorr(ggcorr_anime_data, geom = "blank", label = TRUE, hjust = 0.75) +
  geom_point(size = 10, aes(color = coefficient > 0, alpha = abs(coefficient) > 0.5)) +
  scale_alpha_manual(values = c("TRUE" = 0.25, "FALSE" = 0)) +
  guides(color = FALSE, alpha = FALSE) +
  labs(
    title = "An overview of the correlations of our variables",
    subtitle = "Slight correlations with the score and stronger correlations with the droprate are detectable",
    caption = "Source ID"
  ) +
  theme(text = element_text(family = "Times New Roman"))

ggcor_anime

rm(ggcor_anime_data)


```

### Anime movies per genre

```{r anime movies per genre}
#Most anime movies per genres
count_by_genre <- animelist_clean_long %>% 
  group_by(genre) %>% 
  summarise(count = n()) %>% 
  slice_max(count, n = 20) %>% 
  mutate(genre = str_replace(genre, "_", " ")) %>% 
  mutate(genre = str_to_title(genre))
plot1 <- ggplot(data = count_by_genre, aes(x = count, y = fct_reorder(genre, count))) +
  geom_col(fill= '#0056a7') +
  labs(
    title = "Top 20 represented Anime Genres",
    x = "Number of animes") +
  theme_classic(base_family = "Times New Roman") +
  theme(plot.title.position = "plot",
        plot.title = element_textbox_simple(size = 14),
        axis.title.y = element_blank(),
        legend.position = "none"
        )
plot1
rm(count_by_genre)
```

### 

### Ratings per genre and dropout rates per genre

```{r ratings per genre and dropout rates per genre}
#Ratings per genre and drop out rate per genre
ratingdropped_by_genre <- animelist_clean_long %>% 
  group_by(genre) %>% 
  summarise(rating = mean(score),
            droprate = mean(100*dropped/(dropped+watching+completed+on_hold))) %>%
  slice_max(rating, n = 20) %>%
  mutate(genre = str_replace(genre, "_", " ")) %>% 
  mutate(genre = str_to_title(genre))

library(HH) # -,,-
plot2 <- likert(genre ~ ., ratingdropped_by_genre, main="The relationship between the rating and the droprate",
       xlab="Rating vs. Droprate in %",
       ylab="",
       xlim=c(-10,10),
       scales=list(x=list(at=seq(-10,10,5),
                          labels=c(seq(10,0,-5),seq(5,10,5)))))
plot(plot2, legend.position="right") #cannot seem to get the legend to the right

rm(ratingdropped_by_genre)
detach("package:HH", unload=TRUE)

# plot2 <- ggplot(data = rating_by_genre, aes(x = rating, y = fct_reorder(genre, rating))) +
#   geom_col() +
#   labs(
#     title = "Top 20 average ratings per Anime Genres",
#     x = "Rating of animes") +
#   theme_classic(base_family = "Times New Roman") +
#   theme(plot.title.position = "plot",
#         plot.title = element_textbox_simple(size = 14),
#         axis.title.y = element_blank(),
#         legend.position = "none",
#         )
# plot(plot2)
# rm(rating_by_genre)

```

### Watch time per genre

```{r watch time per genre}
#get the top 20 watched genre's
most_watched <- animelist_clean_long %>% 
  dplyr::select(genre,type,duration,episodes,completed) %>% 
  group_by(genre) %>% 
  summarise(counter = sum(duration*episodes*completed, na.rm = TRUE)) %>% 
  top_n(counter, n = 20)

#calculate
watchtime_by_genre <- animelist_clean_long %>% 
  filter(genre %in% most_watched$genre) %>% 
  dplyr::select(genre,type,duration,episodes,completed) %>% 
  group_by(genre) %>% 
  mutate(watch_time_genre = sum(duration*episodes*completed, na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(genre, type) %>% 
  summarise(watch_time_genretype = sum(duration*episodes*completed, na.rm = TRUE),
            watch_time_genre = mean(watch_time_genre)) %>%
  # slice_max(watch_time_genre, n = 20) %>%
  mutate(genre = str_replace(genre, "_", " ")) %>% 
  mutate(genre = str_to_title(genre))
  
plot3 <- ggplot(data = watchtime_by_genre, aes(x = watch_time_genretype, y = fct_reorder(genre, watch_time_genre), fill = type)) +
  geom_col() +
  labs(
    title = "20 most watched Anime genres by type",
    x = "Watchtime of animes",
    fill = "Type") +
  theme_classic(base_family = "Times New Roman") +
  theme(plot.title.position = "plot",
        plot.title = element_textbox_simple(size = 14),
        axis.title.y = element_blank(),
        ) +
  scale_fill_manual(values = c('#aeecfc', '#8bd7f5', '#52ace0', '#3a97d3' ,'#126cb6' , '#0056a7'))
plot(plot3)
rm(watchtime_by_genre)


```

### Rating per average length of a movie per type

```{r rating per average length of a movie per type}
#Rating per average length of a movie per type
rating_by_duration <- animelist_clean_long %>% 
  dplyr::select(type,duration,episodes, score) %>% 
  mutate(total_duration = duration*episodes) %>% 
  group_by(score, type) %>% 
  summarise(mean_duration = mean(total_duration))

plot4 <- ggplot(rating_by_duration, aes(x = score, y = mean_duration)) +
  geom_point() +
  facet_wrap(~ type, scales = "free") +
  geom_smooth(aes(colour = type), method = "lm", se = FALSE, colour = '#0056a7') +
  xlim(0,10) +
  labs(
    title = "An increase in animes' duration increases its rating",
    x = "Rating of animes") +
  theme_classic(base_family = "Times New Roman") +
  theme(plot.title.position = "plot",
        plot.title = element_text(size = 14),
        axis.title.y = element_blank(),
        legend.position = "none",
        ) 
plot4


```

## Machine Learning Techniques

### User recommendation

### Predicting Anime drop rates

```{r creating data set for LASSO}
#split the data in testing and training.
animelist_dropped <- animelist_clean %>% 
  relocate(droprate, .before = mal_id) %>% 
  dplyr::select(droprate, c(score : genre_yuri)) %>% 
  dplyr::select(-c(watching : on_hold)) %>%  #Dropped watching to on hold because these were used to calculate droprates
  drop_na(.) %>% #Drop na values for analysis
  filter(source != "Unknown") %>% 
  filter(rating != "Unknown")

skim(animelist_dropped)

unknown_count <- as.data.frame(sapply(animelist_dropped, function(x) sum(x == "Unknown"))) %>% 
  rownames_to_column(., "name")
NA_count <- as.data.frame(sapply(animelist_dropped, function(x) sum(is.na(x))))
unknown_NA_count <- cbind(unknown_count, NA_count) %>% 
  # lapply(., function(x) as.numeric(x)) %>%
  # as.data.frame(.) %>% 
  mutate_all(~replace(., is.na(.), 0))
colnames(unknown_NA_count) <- c("name", "unknown", "NA")

overall_invalid_count <- unknown_NA_count %>% 
  mutate(overall = unknown + `NA`)

set.seed(1234)
train_test_split <- initial_split(animelist_dropped, prop = 0.7)
training <- training(train_test_split)
testing <- testing(train_test_split)


# we will look for the optimal lambda in this sequence (we will try 1000 different lambdas, feel free to try more if necessary)
lambda_seq <- seq(0, 0.01, length = 1000)


#the method "cv" stands for cross validation. We re going to create 10 folds.  
control <- trainControl (
    method="cv",
    number=10,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

```

### LASSO droprate on all

```{r LASSO dropped on all}
# lasso regression using k-fold cross validation to select the best lambda
lasso_dropped_all <- train(
 droprate ~ .,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression.
  )

# Model coefficients
nonzerocoef<-data.frame(as.matrix(coef(lasso_dropped_all$finalModel, lasso_dropped_all$bestTune$lambda)))

# Best lambda
lasso_dropped_all$bestTune$lambda

# Count of how many coefficients are greater than zero and how many are equal to zero
sum(coef(lasso_dropped_all$finalModel, lasso_dropped_all$bestTune$lambda)!=0)
sum(coef(lasso_dropped_all$finalModel, lasso_dropped_all$bestTune$lambda)==0)

# Make predictions
predictions_dropped_all <- predict(lasso_dropped_all,testing) %>% 
  cbind(.,
        testing %>% dplyr::select(c(droprate, type))
        )

colnames(predictions_dropped_all) <- c("predicted", "droprate", "type")
predictions_dropped_all <- predictions_dropped_all %>% 
  mutate(error = predicted - droprate)

# Plot predictions
ggplot(predictions_dropped_all, aes(x = droprate, y = predicted))+
  geom_point(aes(colour = type), alpha = 0.5) +
  geom_smooth(method = "loess", colour = "red", se = F) +
  geom_abline(intercept = 0, slope = 1, colour = "black") +
  labs(
    title = "<b>Predicted droprate on Actual droprate (All variables as predictors)</b>",
    x = "Actual droprate",
    y = "Predicted droprate",
    colour = "Type") +
  theme_minimal() +
  theme(plot.title.position = "plot",
        plot.title = element_textbox_simple(size = 14)
        )

# Plot predictions (faceted on type)
ggplot(predictions_dropped_all, aes(x = droprate, y = predicted))+
  geom_point(aes(colour = type), alpha = 0.5) +
  geom_smooth(method = "loess", colour = "red", se = F) +
  geom_abline(intercept = 0, slope = 1, colour = "black") +
  facet_wrap(~type, scales = "free") +
  labs(
    title = "<b>Predicted droprate on Actual droprate (All variables as predictors)</b>",
    x = "Actual droprate",
    y = "Predicted droprate",
    colour = "Type") +
  theme_minimal() +
  theme(plot.title.position = "plot",
        plot.title = element_textbox_simple(size = 14),
        legend.position = "none"
        )

# Model prediction performance
data.frame(
  RMSE = RMSE(predictions_dropped_all$predicted,
              predictions_dropped_all$droprate),
  Rsquare = R2(predictions_dropped_all$predicted,
              predictions_dropped_all$droprate)
  )
```

### LASSO dropped on genres

```{r LASSO dropped on genres}
animelist_dropped_genres <- animelist_dropped %>% 
  dplyr::select(droprate, c(genre_action : genre_yuri))

set.seed(1234)
train_test_split <- initial_split(animelist_dropped_genres, prop = 0.7)
training <- training(train_test_split)
testing <- testing(train_test_split)


# we will look for the optimal lambda in this sequence (we will try 1000 different lambdas, feel free to try more if necessary)
lambda_seq <- seq(0, 0.01, length = 1000)

# lasso regression using k-fold cross validation to select the best lambda
lasso_dropped_genre <- train(
 droprate ~ .,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression.
  )

# Model coefficients
nonzerocoef<-data.frame(as.matrix(coef(lasso_dropped_genre$finalModel, lasso_dropped_genre$bestTune$lambda)))

# Best lambda
lasso_dropped_genre$bestTune$lambda

# Count of how many coefficients are greater than zero and how many are equal to zero
sum(coef(lasso_dropped_genre$finalModel, lasso_dropped_genre$bestTune$lambda)!=0)
sum(coef(lasso_dropped_genre$finalModel, lasso_dropped_genre$bestTune$lambda)==0)

# Make predictions
predictions_dropped_genre <- predict(lasso_dropped_genre,testing) %>% 
  cbind(.,
        testing %>% select(c(dropped))
        ) 

colnames(predictions_dropped_genre) <- c("predicted", "dropped")
predictions_dropped_genre <- predictions_dropped_genre %>% 
  mutate(error = predicted - dropped)

ggplot(predictions_dropped_genre, aes(x = dropped, y = predicted))+
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", colour = "red", se = F) +
  geom_abline(intercept = 0, slope = 1, colour = "black") +
  labs(
    title = "<b>Predicted dropped on Actual dropped (Only genres as predictors)</b>",
    x = "Actual dropped",
    y = "Predicted dropped") +
  theme_minimal() +
  theme(plot.title.position = "plot",
        plot.title = element_textbox_simple(size = 14),
        legend.position = "none"
        )

# Model prediction performance
data.frame(
  RMSE = RMSE(predictions_dropped_genre$predicted,
              predictions_dropped_genre$dropped),
  Rsquare = R2(predictions_dropped_genre$predicted,
              predictions_dropped_genre$dropped)
  )

```

### Predicting Anime scores

```{r Creating score dataset}
#split the data in testing and training.

animelist_score <- animelist_clean %>% 
  select(c(score : plan_to_watch), c(aired_from_year : premier_year), c(genre_action : genre_yuri)) %>% 
  select(-c(premier_season, premier_year)) %>%  #Dropped premier beccause there are too many NAs/Unknowns
  drop_na(.) %>% #Drop na values for analysis
  filter(source != "Unknown") %>% 
  filter(rating != "Unknown") %>% 
  # levels(droplevels(animelist_dropped$source)) %>% 
  select(-c(source)) %>%  #Drop source for convenience
  select(-c(ranked, popularity)) #Drop ranked, popularity

skim(animelist_score)

unknown_count <- as.data.frame(sapply(animelist_score, function(x) sum(x == "Unknown"))) %>% 
  rownames_to_column(., "name")
NA_count <- as.data.frame(sapply(animelist_score, function(x) sum(is.na(x))))
unknown_NA_count <- cbind(unknown_count, NA_count) %>% 
  # lapply(., function(x) as.numeric(x)) %>%
  # as.data.frame(.) %>% 
  mutate_all(~replace(., is.na(.), 0))
colnames(unknown_NA_count) <- c("name", "unknown", "NA")

overall_invalid_count <- unknown_NA_count %>% 
  mutate(overall = unknown + `NA`)

set.seed(1234)
train_test_split <- initial_split(animelist_score, prop = 0.7)
training <- training(train_test_split)
testing <- testing(train_test_split)


# we will look for the optimal lambda in this sequence (we will try 1000 different lambdas, feel free to try more if necessary)
lambda_seq <- seq(0, 0.01, length = 1000)


#the method "cv" stands for cross validation. We re going to create 10 folds.  
control <- trainControl (
    method="cv",
    number=10,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation
```

### LASSO score on all

```{r LASSO score on all}
# lasso regression using k-fold cross validation to select the best lambda
lasso_score_all <- train(
 score ~ .,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression.
  )

# Model coefficients
nonzerocoef<-data.frame(as.matrix(coef(lasso_score_all$finalModel, lasso_score_all$bestTune$lambda)))

# Best lambda
lasso_score_all$bestTune$lambda

# Count of how many coefficients are greater than zero and how many are equal to zero
sum(coef(lasso_score_all$finalModel, lasso_score_all$bestTune$lambda)!=0)
sum(coef(lasso_score_all$finalModel, lasso_score_all$bestTune$lambda)==0)

# Make predictions
predictions_score_all <- predict(lasso_score_all,testing) %>% 
  cbind(.,
        testing %>% select(c(score,type))
        ) 

colnames(predictions_score_all) <- c("predicted", "score", "type")
predictions_score_all <- predictions_score_all %>% 
  mutate(error = predicted - score)

#Plot prediction outcomes
ggplot(predictions_score_all, aes(x = score, y = predicted))+
  geom_point(aes(colour = type), alpha = 0.5) +
  geom_smooth(method = "loess", colour = "red", se = F) +
  geom_abline(intercept = 0, slope = 1, colour = "black", linetype = "dashed") +
  labs(
    title = "<b>Predicted score on Actual score (All variables as predictors)</b>",
    x = "Actual score",
    y = "Predicted score",
    colour = "Type") +
  theme_minimal() +
  theme(plot.title.position = "plot",
        plot.title = element_textbox_simple(size = 14)
        )

#Plot prediction outcomes (faceted)
ggplot(predictions_score_all, aes(x = score, y = predicted))+
  geom_point(aes(alpha = 0.5, colour = type)) +
  geom_smooth(method = "loess", colour = "red", se = F) +
  geom_abline(intercept = 0, slope = 1, colour = "black", linetype = "dashed") +
  labs(
    title = "<b>Predicted score on Actual score (All variables as predictors)</b>",
    x = "Actual score",
    y = "Predicted score") +
  facet_wrap(~type) +
  theme_minimal() +
  theme(plot.title.position = "plot",
        plot.title = element_textbox_simple(size = 14),
        legend.position = "none"
        )

# Model prediction performance
data.frame(
  RMSE = RMSE(predictions_score_all$predicted, predictions_score_all$score),
  Rsquare = R2(predictions_score_all$predicted, predictions_score_all$score)
)
```

### LASSO score on genres

```{r LASSO score on genres}
animelist_score_genres <- animelist_score %>% 
  select(score, c(genre_action : genre_yuri))

set.seed(1234)
train_test_split <- initial_split(animelist_score_genres, prop = 0.7)
training <- training(train_test_split)
testing <- testing(train_test_split)


# we will look for the optimal lambda in this sequence (we will try 1000 different lambdas, feel free to try more if necessary)
lambda_seq <- seq(0, 0.01, length = 1000)


# lasso regression using k-fold cross validation to select the best lambda
lasso_score_genre <- train(
 score ~ .,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression.
  )

# Model coefficients
nonzerocoef<-data.frame(as.matrix(coef(lasso_score_genre$finalModel, lasso_score_genre$bestTune$lambda)))

# Best lambda
lasso_score_genre$bestTune$lambda

# Count of how many coefficients are greater than zero and how many are equal to zero
sum(coef(lasso_score_genre$finalModel, lasso_score_genre$bestTune$lambda)!=0)
sum(coef(lasso_score_genre$finalModel, lasso_score_genre$bestTune$lambda)==0)

# Make predictions
predictions_score_genre <- predict(lasso_score_genre,testing) %>% 
  cbind(.,
        testing %>% select(c(score))
        ) 

colnames(predictions_score_genre) <- c("predicted", "score")
predictions_score_genre <- predictions_score_genre %>% 
  mutate(error = predicted - score)

# Plot prediction outcomes
ggplot(predictions_score_genre, aes(x = score, y = predicted))+
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", colour = "red", se = F) +
  geom_abline(intercept = 0, slope = 1, colour = "black", linetype = "dashed") +
  labs(
    title = "<b>Predicted score on Actual score (Only genres as predictors)</b>",
    x = "Actual score",
    y = "Predicted score") +
  theme_minimal() +
  theme(plot.title.position = "plot",
        plot.title = element_textbox_simple(size = 14)
        )

# Model prediction performance
data.frame(
  RMSE = RMSE(predictions_score_genre$predicted,
              predictions_score_genre$score),
  Rsquare = R2(predictions_score_genre$predicted,
              predictions_score_genre$score)
  )


```

### Clustering and PCA

```{r, prepare data for clustering}
#### focus only on tasting points and so ignore geographical coordinates
animelist_clean_genres<- animelist_clean %>% select(genre_action : genre_yuri)
#### scale data: By scaling we substract the average value of a column from each observation and divide each observation by the standard deviation of the column it belongs to
animelist_clean_genres<-data.frame(scale(animelist_clean_genres))
```

```{r kmeans-elbow, message=FALSE, warning=FALSE}
library(purrr) #a package for writing succinctfor loops

# Use map_dbl to run K-Means models with varying value of k 
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = animelist_clean_genres, centers = k,iter.max = 100, nstart = 10)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10 ,
  tot_withinss = tot_withinss
)

# Plot the elbow plot
ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  geom_line() +
  scale_x_continuous(breaks = 1:10)

#Here is a short way of producing the elbow chart using "fviz_nbclust" function. 
fviz_nbclust(animelist_clean_genres,kmeans, method = "wss")+
  labs(subtitle = "Elbow method")
```

```{r kmeans-PCA number of clusters, message=FALSE, warning=FALSE}
# eclust function (part of factoextra) makes it easier to visuliaze clustering results
model_km2 <- eclust(animelist_clean_genres, "kmeans", k = 2,nstart = 50, graph = FALSE)
model_km2$size
model_km3 <- eclust(animelist_clean_genres, "kmeans", k = 3,nstart = 50, graph = FALSE)
model_km3$size
model_km4 <- eclust(animelist_clean_genres, "kmeans", k = 4,nstart = 50, graph = FALSE)
model_km4$size
model_km5 <- eclust(animelist_clean_genres, "kmeans", k = 5,nstart = 50, graph = FALSE)
model_km5$size


# plots to compare
#I use the fviz_cluster function which is part of the`factoextra` library
p1 <- fviz_cluster(model_km2, geom = "point", data = animelist_clean_genres) + ggtitle("k = 2")
p2 <- fviz_cluster(model_km3, geom = "point",  data = animelist_clean_genres) + ggtitle("k = 3")
p3 <- fviz_cluster(model_km4, geom = "point",  data = animelist_clean_genres) + ggtitle("k = 4")
p4 <- fviz_cluster(model_km5, geom = "point",  data = animelist_clean_genres) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

```{r centre plots for k = 1:5}
#Plot centers

#Plot centers for k=2
xa<-data.frame(cluster=as.factor(c(1:2)),model_km2$centers)
xak2<-xa %>% gather(variable,value,-cluster,factor_key = TRUE)

graphknn2<-ggplot(xak2, aes(x = variable, y = value))+  geom_line(aes(color =cluster,group = cluster), linetype = "dashed",size=1)+ geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),legend.title=element_text(size=5),legend.text = element_text(size=5))+ggtitle("K-means Centers k=2")

#Plot centers for k=3
xa<-data.frame(cluster=as.factor(c(1:3)),model_km3$centers)
xa3<-xa %>% gather(variable,value,-cluster,factor_key = TRUE)

graphknn3<-ggplot(xa3, aes(x = variable, y = value))+  geom_line(aes(color =cluster,group = cluster), linetype = "dashed",size=1)+ geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),legend.title=element_text(size=5),legend.text = element_text(size=5))+ggtitle("K-means Centers k=3")

#Plot centers for k=4
xa<-data.frame(cluster=as.factor(c(1:4)),model_km4$centers)

xa4<-xa %>% gather(variable,value,-cluster,factor_key = TRUE)
graphknn4<-ggplot(xa4, aes(x = variable, y = value))+  geom_line(aes(color = cluster,group = cluster), linetype = "dashed",size=1)+ geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),legend.title=element_text(size=5),legend.text = element_text(size=5))+ggtitle("K-means Centers k=4")

#Plot centers for k=5
xa<-data.frame(cluster=as.factor(c(1:5)),model_km5$centers)

xa5<-xa %>% gather(variable,value,-cluster,factor_key = TRUE)
graphknn5<-ggplot(xa5, aes(x = variable, y = value))+  geom_line(aes(color = cluster,group = cluster), linetype = "dashed",size=1)+ geom_point(size=1,shape=4)+geom_hline(yintercept=0)+theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),legend.title=element_text(size=5),legend.text = element_text(size=5))+ggtitle("K-means Centers k=5")
  
model_km2$size
model_km3$size
model_km4$size
model_km5$size


grid.arrange(graphknn2, graphknn3,graphknn4,graphknn5, nrow = 2)
```

```{r silhouette plots}
s2<-fviz_silhouette(model_km2)+ ggtitle(paste("k = 2", "avg sw=",format(round(model_km2$silinfo$avg.width,3))))
s3<-fviz_silhouette(model_km3)+ ggtitle(paste("k = 3", "avg sw=",format(round(model_km3$silinfo$avg.width,3))))
s4<-fviz_silhouette(model_km4)+ ggtitle(paste("k = 4", "avg sw=",format(round(model_km4$silinfo$avg.width,3))))
s5<-fviz_silhouette(model_km5)+ ggtitle(paste("k = 5", "avg sw=",format(round(model_km5$silinfo$avg.width,3))))
grid.arrange(s2, s3,s4,s5, nrow = 2)

```

### Create 10 top anime genre by production

```{r top 10 anime genres dataset}
top_10_genre <- animelist_clean_long %>% 
  group_by(genre) %>% 
  summarise(count = n()) %>% 
  slice_max(count, n = 10) %>% 
  arrange(count) %>% 
  select(-count) %>% 
  unlist()

animelist_top10 <- animelist_clean_long %>% 
  filter(genre %in% top_10_genre) %>% 
  pivot_wider(names_from = genre, values_from = binary) %>%
  mutate_at(vars(action:school), ~replace_na(.,0)) %>% 
  select(action : school)


model_km3 <- eclust(animelist_top10, "kmeans", k = 3,nstart = 50, graph = FALSE)
p2 <- fviz_cluster(model_km3, geom = "point",  data = animelist_top10) + ggtitle("k = 3")
p2
```

### Cluster on top 10 genres

```{r cluster on top 10}
# eclust function (part of factoextra) makes it easier to visuliaze clustering results
model_km2 <- eclust(animelist_clean_genres, "kmeans", k = 2,nstart = 50, graph = FALSE)
model_km2$size
model_km3 <- eclust(animelist_clean_genres, "kmeans", k = 3,nstart = 50, graph = FALSE)
model_km3$size
model_km4 <- eclust(animelist_clean_genres, "kmeans", k = 4,nstart = 50, graph = FALSE)
model_km4$size
model_km5 <- eclust(animelist_clean_genres, "kmeans", k = 5,nstart = 50, graph = FALSE)
model_km5$size


# plots to compare
#I use the fviz_cluster function which is part of the`factoextra` library
p1 <- fviz_cluster(model_km2, geom = "point", data = animelist_clean_genres) + ggtitle("k = 2")
p2 <- fviz_cluster(model_km3, geom = "point",  data = animelist_clean_genres) + ggtitle("k = 3")
p3 <- fviz_cluster(model_km4, geom = "point",  data = animelist_clean_genres) + ggtitle("k = 4")
p4 <- fviz_cluster(model_km5, geom = "point",  data = animelist_clean_genres) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```
